# -*- coding: utf-8 -*-
"""All Genres.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MEhU7XNADA_-HqRhzVaZGzA3KEV1gxXs
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile
# extract
extract_zip = '/content/drive/Shared drives/GrumpyNumpyPandas/neural_networks.zip'
zip_ref = zipfile.ZipFile(extract_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

base_dir = '/content/neural_networks'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'test')

# Training Directories
train_3_dir = os.path.join(train_dir, 'train_3')
train_5_dir = os.path.join(train_dir, 'train_5')
train_6_dir = os.path.join(train_dir, 'train_6')
train_7_dir = os.path.join(train_dir, 'train_7')
train_8_dir = os.path.join(train_dir, 'train_8')
train_9_dir = os.path.join(train_dir, 'train_9')
train_10_dir = os.path.join(train_dir, 'train_10')
train_11_dir = os.path.join(train_dir, 'train_11')
train_12_dir = os.path.join(train_dir, 'train_12')
train_13_dir = os.path.join(train_dir, 'train_13')
train_14_dir = os.path.join(train_dir, 'train_14')
train_16_dir = os.path.join(train_dir, 'train_16')
train_17_dir = os.path.join(train_dir, 'train_17')

# Test Directories
validation_3_dir = os.path.join(validation_dir, 'test_3')
validation_5_dir = os.path.join(validation_dir, 'test_5')
validation_6_dir = os.path.join(validation_dir, 'test_6')
validation_7_dir = os.path.join(validation_dir, 'test_7')
validation_8_dir = os.path.join(validation_dir, 'test_8')
validation_9_dir = os.path.join(validation_dir, 'test_9')
validation_10_dir = os.path.join(validation_dir, 'test_10')
validation_11_dir = os.path.join(validation_dir, 'test_11')
validation_12_dir = os.path.join(validation_dir, 'test_12')
validation_13_dir = os.path.join(validation_dir, 'test_13')
validation_14_dir = os.path.join(validation_dir, 'test_14')
validation_16_dir = os.path.join(validation_dir, 'test_16')
validation_17_dir = os.path.join(validation_dir, 'test_17')

#build architecture before preprocessing
from tensorflow.keras import layers
from tensorflow.keras import Model

img_input = layers.Input(shape=(150, 150, 3))

x = layers.Conv2D(16, 3, activation='relu')(img_input)
x = layers.MaxPooling2D(2)(x)

x = layers.Conv2D(32, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

x = layers.Conv2D(64, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

x = layers.Flatten()(x)

x = layers.Dense(512, activation='relu')(x)

#line that will change for if not binary
output = layers.Dense(13, activation='softmax')(x)

model = Model(img_input, output)

from tensorflow.keras.optimizers import RMSprop

#line also change if not binary
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['acc'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# rescale by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# training set generator
# change class_mode?
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=50,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='categorical')

# test set
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=50,
        class_mode='categorical')

history = model.fit_generator(
      train_generator,
      steps_per_epoch=87,  # 4050 images = batch_size * steps
      epochs=15,
      validation_data=validation_generator,
      validation_steps=32,  # 1500 images = batch_size * steps
      verbose=2)

import matplotlib.pyplot as plt
# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['acc']
val_acc = history.history['val_acc']

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc)
plt.plot(epochs, val_acc)
plt.title('Training and validation accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss)
plt.plot(epochs, val_loss)
plt.title('Training and validation loss')